# DIY Neural Network

## Session A: Real-Time Data

### Objectives:
* Evaluate and critique models trained in week 4 assignment.
* Compare training and inference for real-time interaction with more traditional machine learning pipelines.

### Pose Tracking
* [PoseNet all points](https://editor.p5js.org/ima_ml/sketches/ZZtfEKugW)

### Face Tracking
* Coming soon...

### p5.js Oscillators
* [p5.js Oscillator example](https://editor.p5js.org/ima_ml/sketches/fSGClc_aK)
* [Sound Synthesis video tutorial](https://youtu.be/Bk8rLzzSink)

### Related projects that map gesture to sound
* [MARtLET](https://vimeo.com/19980514) by Michelle Nagai
* [From the Waters](https://www.youtube.com/watch?v=k6dwnr5RDow) by Anne Hege
* [This Is Not A Theremin](https://sofiaitp.wordpress.com/2018/12/04/this-is-not-a-theremin/) by Guillermo Montecinos and Sof√≠a Suazo
* [Eye Conductor](https://andreasrefsgaard.dk/project/eye-conductor/) by Andreas Refsgaard

### Reading / Viewing
* [Machine Learning for Humaan Creative Practice](https://vimeo.com/287094397), Dr. Rebecca Fiebrink at Eyeo 2018 |]

## Session B: Training the Model

### Objectives:
* Revisit and examine the concepts of classification and regression as applied to real-time interaction.

### Examples
* [Train model with Mouse](https://editor.p5js.org/ima_ml/sketches/eW8o-mYJf)
* [Train model with Pixels](https://editor.p5js.org/ima_ml/sketches/EMDiQlIhV)
* [Train model with Face Keypoints](https://editor.p5js.org/ima_ml/sketches/US3ZX6zCD)

### Assignment 5 Due Sunday October 6 at 6pm
* Design the inputs and outputs of a real-time machine learning system for interaction and audio/visual performance. Write a short blog post describing the inputs and outputs of your system.
* Create your own sound or visuals generator by training a machine learning model on inputs generated from interaction (mouse, face movements, gesture). This can be a prototype of the idea of a simple exercises building on the examplese listed above.
    * suggested exercises TBA
