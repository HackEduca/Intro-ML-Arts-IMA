# DIY Neural Network

## Session A: Real-Time Data

### Objectives:
* Evaluate and critique models trained in week 4 assignment.
* Compare training and inference for real-time interaction with more traditional machine learning pipelines.

### Pose Tracking


### Face Tracking

### p5.js Sound

### Related projects that map gesture to sound
* [MARtLET](https://vimeo.com/19980514) by Michelle Nagai
* [From the Waters](https://www.youtube.com/watch?v=k6dwnr5RDow) by Anne Hege
* [This Is Not A Theremin](https://sofiaitp.wordpress.com/2018/12/04/this-is-not-a-theremin/) by Guillermo Montecinos and Sof√≠a Suazo
* [Eye Conductor](https://andreasrefsgaard.dk/project/eye-conductor/) by Andreas Refsgaard

## Session B: Training the Model

### Objectives:
* Revisit and examine the concepts of classification and regression as applied to real-time interaction.

### Examples
* browser-based musical instrument with the mouse, a neural network classifier, and p5.sound.js.
* replace mouse with hand gesture using the PoseNet model.
* face-api.js

### Assignment 5 Due Sunday October 6 at 6pm
* Design the inputs and outputs of a real-time machine learning system for interaction and audio/visual performance. Write a short blog post describing the inputs and outputs of your system.
* Create your own sound or visuals generator by training a machine learning model on inputs generated from interaction (mouse, face movements, gesture).
